{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks, butter, filtfilt\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def H(title):\n",
    "    line = \"=\" * max(12, len(title))\n",
    "    print(\"\\n\" + line)\n",
    "    print(title)\n",
    "    print(line)\n",
    "\n",
    "H(\"Imports and runtime state\")\n",
    "print(\"Libraries loaded\")\n",
    "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "H(\"Global parameters and core assumptions\")\n",
    "RESP_H5_PATH = r\"C:\\Users\\Padilla-Coreano\\Documents\\tanish\\Resp_h5\"\n",
    "EXCEL_PATH = r\"C:\\Users\\Padilla-Coreano\\Documents\\tanish\\Keypoint RI1+RI2\\Excel_Results\"\n",
    "PROCESSED_DATA_PATH = r\"C:\\Users\\Padilla-Coreano\\Documents\\tanis...oint RI1+RI2\\Excel_Results\\processed_respiration_data_0.5Hz.pkl\"\n",
    "\n",
    "ORIGINAL_SAMPLING_RATE = 20000\n",
    "TARGET_SAMPLING_RATE = 100\n",
    "MIN_SYLLABLE_DURATION = 500\n",
    "\n",
    "PEAK_MIN_DISTANCE = 0.08\n",
    "PEAK_PROMINENCE_FACTOR = 0.2\n",
    "\n",
    "FPS_MAPPING = {\n",
    "    \"RI1_s1_1\": 15,\n",
    "    \"RI1_s2_3\": 15,\n",
    "    \"RI1_s3_6\": 29,\n",
    "    \"RI2_s1_1\": 15,\n",
    "    \"RI2_s2_4\": 15\n",
    "}\n",
    "DEFAULT_FPS = 30\n",
    "\n",
    "print({\n",
    "    \"ORIGINAL_SAMPLING_RATE\": ORIGINAL_SAMPLING_RATE,\n",
    "    \"TARGET_SAMPLING_RATE\": TARGET_SAMPLING_RATE,\n",
    "    \"MIN_SYLLABLE_DURATION_MS\": MIN_SYLLABLE_DURATION,\n",
    "    \"PEAK_MIN_DISTANCE_SEC\": PEAK_MIN_DISTANCE,\n",
    "    \"PEAK_PROMINENCE_FACTOR\": PEAK_PROMINENCE_FACTOR,\n",
    "    \"DEFAULT_FPS\": DEFAULT_FPS,\n",
    "    \"FPS_MAPPING_KEYS\": sorted(list(FPS_MAPPING.keys()))\n",
    "})\n",
    "\n",
    "H(\"H5 ingestion primitives\")\n",
    "def list_h5_datasets(h5_path):\n",
    "    keys = []\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        def visitor(name, obj):\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                keys.append(name)\n",
    "        f.visititems(visitor)\n",
    "    return sorted(keys)\n",
    "\n",
    "def read_h5_signal(h5_path, dataset_key):\n",
    "    with h5py.File(h5_path, \"r\") as f:\n",
    "        x = np.array(f[dataset_key])\n",
    "    return x\n",
    "\n",
    "print(\"H5 dataset listing and reading ready\")\n",
    "\n",
    "H(\"Filtering and resampling primitives\")\n",
    "def butter_lowpass(cutoff_hz, fs_hz, order=4):\n",
    "    nyq = 0.5 * fs_hz\n",
    "    normal = cutoff_hz / nyq\n",
    "    b, a = butter(order, normal, btype=\"low\", analog=False)\n",
    "    return b, a\n",
    "\n",
    "def apply_lowpass(x, cutoff_hz, fs_hz, order=4):\n",
    "    b, a = butter_lowpass(cutoff_hz, fs_hz, order=order)\n",
    "    return filtfilt(b, a, x)\n",
    "\n",
    "def resample_by_decimation(x, original_fs, target_fs):\n",
    "    factor = int(round(original_fs / target_fs))\n",
    "    if factor <= 0:\n",
    "        raise ValueError(\"Invalid decimation factor\")\n",
    "    if original_fs % target_fs != 0:\n",
    "        raise ValueError(\"original_fs must be an integer multiple of target_fs for this resampling method\")\n",
    "    return x[::factor]\n",
    "\n",
    "print(\"Filter + decimation resampling ready\")\n",
    "\n",
    "H(\"Normalization primitives (if used before peak detection)\")\n",
    "def zscore(x):\n",
    "    s = np.std(x)\n",
    "    m = np.mean(x)\n",
    "    if s == 0:\n",
    "        return x * 0\n",
    "    return (x - m) / s\n",
    "\n",
    "def center(x):\n",
    "    return x - np.mean(x)\n",
    "\n",
    "print(\"Normalization helpers ready\")\n",
    "\n",
    "H(\"Peak detection logic\")\n",
    "def detect_breathing_peaks(signal, fs=TARGET_SAMPLING_RATE):\n",
    "    signal_std = np.std(signal)\n",
    "    signal_mean = np.mean(signal)\n",
    "\n",
    "    peaks, properties = find_peaks(\n",
    "        signal,\n",
    "        distance=int(fs * PEAK_MIN_DISTANCE),\n",
    "        prominence=signal_std * PEAK_PROMINENCE_FACTOR,\n",
    "        height=signal_mean\n",
    "    )\n",
    "\n",
    "    return peaks\n",
    "\n",
    "print(\"Peak detection ready\")\n",
    "\n",
    "H(\"Video FPS resolution\")\n",
    "def get_video_fps(filename):\n",
    "    for pattern, fps in FPS_MAPPING.items():\n",
    "        if pattern in filename:\n",
    "            return fps\n",
    "    return DEFAULT_FPS\n",
    "\n",
    "print(\"FPS resolver ready\")\n",
    "\n",
    "H(\"Respiratory rate and IBI math from peaks\")\n",
    "def calculate_respiratory_metrics(peaks, fs=TARGET_SAMPLING_RATE):\n",
    "    if len(peaks) < 2:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "\n",
    "    peak_times = peaks / fs\n",
    "    ibi_seconds = np.diff(peak_times)\n",
    "    breathing_rate_1_over_ibi = 1 / ibi_seconds\n",
    "    breath_times = peak_times[1:]\n",
    "\n",
    "    return breathing_rate_1_over_ibi, breath_times, ibi_seconds\n",
    "\n",
    "print(\"Resp metrics ready\")\n",
    "\n",
    "H(\"Behavior dataframe schema normalization\")\n",
    "def canonicalize_behavior_columns(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    renames = {}\n",
    "    if \"frame_number\" not in df.columns:\n",
    "        if \"Frame\" in df.columns:\n",
    "            renames[\"Frame\"] = \"frame_number\"\n",
    "        if \"frame\" in df.columns:\n",
    "            renames[\"frame\"] = \"frame_number\"\n",
    "\n",
    "    if \"syllable\" not in df.columns:\n",
    "        if \"Syllable\" in df.columns:\n",
    "            renames[\"Syllable\"] = \"syllable\"\n",
    "        if \"syllable_id\" in df.columns:\n",
    "            renames[\"syllable_id\"] = \"syllable\"\n",
    "\n",
    "    if renames:\n",
    "        df = df.rename(columns=renames)\n",
    "\n",
    "    if \"frame_number\" not in df.columns or \"syllable\" not in df.columns:\n",
    "        raise ValueError(f\"Required columns not found after canonicalization. Columns={list(df.columns)}\")\n",
    "\n",
    "    df = df.dropna(subset=[\"frame_number\", \"syllable\"])\n",
    "    df[\"frame_number\"] = df[\"frame_number\"].astype(int)\n",
    "    df[\"syllable\"] = df[\"syllable\"].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"Behavior schema normalizer ready\")\n",
    "\n",
    "H(\"Keypoint-MoSeq syllable bout construction (grouping + duration filtering)\")\n",
    "def create_syllable_bouts(behavior_df, video_fps, min_duration_ms=MIN_SYLLABLE_DURATION):\n",
    "    behavior_df = behavior_df.copy()\n",
    "    behavior_df[\"time_seconds\"] = behavior_df[\"frame_number\"] / video_fps\n",
    "    behavior_df[\"transition\"] = (behavior_df[\"syllable\"] != behavior_df[\"syllable\"].shift()).astype(int)\n",
    "    behavior_df[\"bout_group\"] = behavior_df[\"transition\"].cumsum()\n",
    "\n",
    "    bouts_list = []\n",
    "\n",
    "    for group_id, group in behavior_df.groupby(\"bout_group\"):\n",
    "        syllable_id = int(group[\"syllable\"].iloc[0])\n",
    "\n",
    "        start_time = float(group[\"time_seconds\"].min())\n",
    "        end_time = float(group[\"time_seconds\"].max())\n",
    "        duration_ms = float((end_time - start_time) * 1000)\n",
    "\n",
    "        if duration_ms >= min_duration_ms:\n",
    "            bout_info = {\n",
    "                \"syllable_id\": syllable_id,\n",
    "                \"start_frame\": int(group[\"frame_number\"].min()),\n",
    "                \"end_frame\": int(group[\"frame_number\"].max()),\n",
    "                \"start_time_sec\": start_time,\n",
    "                \"end_time_sec\": end_time,\n",
    "                \"duration_ms\": duration_ms,\n",
    "                \"frame_count\": int(len(group)),\n",
    "                \"fps_used\": float(video_fps),\n",
    "            }\n",
    "            bouts_list.append(bout_info)\n",
    "\n",
    "    return bouts_list\n",
    "\n",
    "print(\"Bout constructor ready\")\n",
    "\n",
    "H(\"Session key reconciliation and Excel selection\")\n",
    "def find_resp_key(resp_dict, session_pattern):\n",
    "    matches = [k for k in resp_dict.keys() if session_pattern in k]\n",
    "    if len(matches) == 0:\n",
    "        return None\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "    matches_sorted = sorted(matches, key=len)\n",
    "    return matches_sorted[0]\n",
    "\n",
    "def pick_excel_file(session_id, excel_files):\n",
    "    matches = [f for f in excel_files if session_id in os.path.basename(f)]\n",
    "    if len(matches) == 0:\n",
    "        return None\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "    matches_sorted = sorted(matches)\n",
    "    return matches_sorted[0]\n",
    "\n",
    "print(\"Key matching helpers ready\")\n",
    "\n",
    "H(\"Windowing math and per-bout respiratory summaries\")\n",
    "def count_peaks_in_window(peak_times, start_time, end_time, inclusive=True):\n",
    "    if inclusive:\n",
    "        mask = (peak_times >= start_time) & (peak_times <= end_time)\n",
    "    else:\n",
    "        mask = (peak_times > start_time) & (peak_times < end_time)\n",
    "    return int(mask.sum())\n",
    "\n",
    "def summarize_bout_respiration(peak_times, breath_times, breathing_rates_hz, start_time, end_time):\n",
    "    peak_count = count_peaks_in_window(peak_times, start_time, end_time, inclusive=True)\n",
    "\n",
    "    mid_time = (start_time + end_time) / 2\n",
    "    if len(breath_times) > 0 and len(breathing_rates_hz) > 0:\n",
    "        rate_hz = float(np.interp(mid_time, breath_times, breathing_rates_hz))\n",
    "        bpm = float(rate_hz * 60)\n",
    "        ibi = float(1 / rate_hz) if rate_hz > 0 else np.nan\n",
    "    else:\n",
    "        rate_hz = np.nan\n",
    "        bpm = np.nan\n",
    "        ibi = np.nan\n",
    "\n",
    "    return {\n",
    "        \"peak_count\": peak_count,\n",
    "        \"mean_breathing_rate_hz\": rate_hz,\n",
    "        \"mean_bpm\": bpm,\n",
    "        \"mean_ibi_sec\": ibi\n",
    "    }\n",
    "\n",
    "print(\"Bout respiration summary ready\")\n",
    "\n",
    "H(\"Single-session integration (peaks + bouts + per-bout features)\")\n",
    "def process_single_session(session_id, respiratory_signal, excel_files):\n",
    "    print(f\"Processing: {session_id}\")\n",
    "\n",
    "    signal = respiratory_signal[\"respiration\"]\n",
    "    fs = respiratory_signal.get(\"fs\", TARGET_SAMPLING_RATE)\n",
    "\n",
    "    peaks = detect_breathing_peaks(signal, fs)\n",
    "    print(f\"  Peaks detected: {len(peaks)}\")\n",
    "\n",
    "    if len(peaks) < 2:\n",
    "        print(\"  Insufficient peaks - skipping\")\n",
    "        return None\n",
    "\n",
    "    breathing_rates_hz, breath_times, ibi_values = calculate_respiratory_metrics(peaks, fs)\n",
    "    peak_times = peaks / fs\n",
    "\n",
    "    excel_file = pick_excel_file(session_id, excel_files)\n",
    "    if excel_file is None:\n",
    "        print(f\"  No Excel file found for {session_id}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"  Using Excel file: {os.path.basename(excel_file)}\")\n",
    "    behavior_df = pd.read_excel(excel_file)\n",
    "    behavior_df = canonicalize_behavior_columns(behavior_df)\n",
    "\n",
    "    video_fps = get_video_fps(os.path.basename(excel_file))\n",
    "    bouts_list = create_syllable_bouts(behavior_df, video_fps, MIN_SYLLABLE_DURATION)\n",
    "\n",
    "    print(f\"  Found {len(bouts_list)} syllable bouts\")\n",
    "\n",
    "    processed_bouts = []\n",
    "    for bout in bouts_list:\n",
    "        start_time = bout[\"start_time_sec\"]\n",
    "        end_time = bout[\"end_time_sec\"]\n",
    "        resp_summary = summarize_bout_respiration(peak_times, breath_times, breathing_rates_hz, start_time, end_time)\n",
    "\n",
    "        out_row = {\n",
    "            \"syllable_id\": bout[\"syllable_id\"],\n",
    "            \"start_time_sec\": start_time,\n",
    "            \"end_time_sec\": end_time,\n",
    "            \"duration_ms\": bout[\"duration_ms\"],\n",
    "            \"frame_count\": bout[\"frame_count\"],\n",
    "            \"fps_used\": bout[\"fps_used\"],\n",
    "        }\n",
    "        out_row.update(resp_summary)\n",
    "        processed_bouts.append(out_row)\n",
    "\n",
    "    results_df = pd.DataFrame(processed_bouts)\n",
    "    results_df = results_df.sort_values(\"start_time_sec\").reset_index(drop=True)\n",
    "\n",
    "    print(f\"  Processed {len(results_df)} syllable bouts\")\n",
    "    return results_df\n",
    "\n",
    "print(\"Single-session runner ready\")\n",
    "\n",
    "H(\"Batch runner and export logic\")\n",
    "def run_complete_analysis():\n",
    "    print(\"Starting analysis...\")\n",
    "\n",
    "    respiratory_data = pd.read_pickle(PROCESSED_DATA_PATH)\n",
    "    excel_files = glob.glob(os.path.join(EXCEL_PATH, \"*track0*.xlsx\"))\n",
    "\n",
    "    print(f\"Found {len(respiratory_data)} respiratory sessions\")\n",
    "    print(f\"Found {len(excel_files)} behavioral files\")\n",
    "\n",
    "    all_results = {}\n",
    "    successful_sessions = 0\n",
    "\n",
    "    for session_id, resp_data in respiratory_data.items():\n",
    "        try:\n",
    "            result = process_single_session(session_id, resp_data, excel_files)\n",
    "\n",
    "            if result is not None:\n",
    "                session_name = session_id.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "                output_file = f\"CORRECTED_PEAK_COUNT_{session_name}_analysis.xlsx\"\n",
    "                result.to_excel(output_file, index=False)\n",
    "                print(f\"  Saved: {output_file}\")\n",
    "\n",
    "                all_results[session_id] = result\n",
    "                successful_sessions += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {session_id}: {str(e)}\")\n",
    "\n",
    "    print(f\"Analysis complete: {successful_sessions}/{len(respiratory_data)} sessions processed\")\n",
    "    return all_results\n",
    "\n",
    "print(\"Batch runner ready\")\n",
    "\n",
    "H(\"Peak-count verification against exported analysis output\")\n",
    "def verify_peak_detection(session_id=\"RI1_s1_1\", test_rows=[2, 5, 8]):\n",
    "    excel_pattern = f\"CORRECTED_PEAK_COUNT_*{session_id}*_analysis.xlsx\"\n",
    "    excel_files = glob.glob(excel_pattern)\n",
    "\n",
    "    if not excel_files:\n",
    "        print(f\"No Excel file found for {session_id}\")\n",
    "        return None\n",
    "\n",
    "    excel_file = excel_files[0]\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "    respiratory_data = pd.read_pickle(PROCESSED_DATA_PATH)\n",
    "    resp_key = find_resp_key(respiratory_data, session_id)\n",
    "    if resp_key is None:\n",
    "        print(f\"No respiration key found for {session_id}\")\n",
    "        return None\n",
    "\n",
    "    signal = respiratory_data[resp_key][\"respiration\"]\n",
    "    fs = respiratory_data[resp_key].get(\"fs\", TARGET_SAMPLING_RATE)\n",
    "\n",
    "    peaks = detect_breathing_peaks(signal, fs)\n",
    "    peak_times = peaks / fs\n",
    "\n",
    "    matches = 0\n",
    "    total_tests = len(test_rows)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for i, row_idx in enumerate(test_rows):\n",
    "        if row_idx >= len(df):\n",
    "            continue\n",
    "\n",
    "        bout = df.iloc[row_idx]\n",
    "        start_time = float(bout[\"start_time_sec\"])\n",
    "        end_time = float(bout[\"end_time_sec\"])\n",
    "\n",
    "        window_mask = (peak_times >= start_time) & (peak_times <= end_time)\n",
    "        window_peak_times = peak_times[window_mask]\n",
    "\n",
    "        graph_count = len(window_peak_times)\n",
    "        excel_count = int(bout[\"peak_count\"])\n",
    "\n",
    "        if graph_count == excel_count:\n",
    "            matches += 1\n",
    "            match_text = \"MATCH\"\n",
    "        else:\n",
    "            match_text = \"MISMATCH\"\n",
    "\n",
    "        plt.subplot(len(test_rows), 1, i + 1)\n",
    "\n",
    "        t = np.arange(len(signal)) / fs\n",
    "        time_window = (t >= start_time - 0.5) & (t <= end_time + 0.5)\n",
    "        window_time = t[time_window]\n",
    "        window_signal = signal[time_window]\n",
    "\n",
    "        plt.plot(window_time, window_signal)\n",
    "\n",
    "        for pt in window_peak_times:\n",
    "            plt.axvline(x=pt, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        plt.axvline(x=start_time, linewidth=2)\n",
    "        plt.axvline(x=end_time, linewidth=2)\n",
    "\n",
    "        plt.title(\n",
    "            f\"Row {row_idx}: Excel={excel_count}, Graph={graph_count} {match_text} \"\n",
    "            f\"(Syllable {int(bout['syllable_id'])}, {start_time:.2f}-{end_time:.2f}s)\"\n",
    "        )\n",
    "\n",
    "    plt.suptitle(f\"{session_id} Verification - {matches}/{total_tests} matches\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Verification: {matches}/{total_tests} syllables match\")\n",
    "    return matches == total_tests\n",
    "\n",
    "print(\"Verification routine ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970de6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
